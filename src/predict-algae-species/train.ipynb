{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "First, we need to clone the codebase:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! git clone https://github.com/mahyar-osn/predict-algae-species.git"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's import the basic libraries:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to append the directory path of the GitHub repo codebase to this notebook first. Then we can load the required\n",
    "modules from our code:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sys.path.insert(0, '/content/predict-algae-species/src/predict-algae-species')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's mount the Google Drive directory, so we can access the data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import the internal modules:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from core.dataloader import SegmentationDataSet\n",
    "from core.model import UNet\n",
    "from core.callbacks import EarlyStopping\n",
    "import core.config as config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import the necessary `PyTorch` modules:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can write `get_dataset()` function to get a specific strain of algae for model training/testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_dataset(strain: str, raw: list, annotation: list):\n",
    "    raw = sorted([x for x in raw if strain in x])\n",
    "    annotation = sorted([x for x in annotation if strain in x])\n",
    "    return raw, annotation\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now need to find the images using the paths provided in the `config` file (imported above) and create a training/testing\n",
    "split:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# load the image and mask filepaths in a sorted manner\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m image_paths \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(\u001B[38;5;28mlist\u001B[39m(\u001B[43mpaths\u001B[49m\u001B[38;5;241m.\u001B[39mlist_images(config\u001B[38;5;241m.\u001B[39mIMAGE_DATASET_PATH)))\n\u001B[0;32m      3\u001B[0m mask_paths \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(\u001B[38;5;28mlist\u001B[39m(paths\u001B[38;5;241m.\u001B[39mlist_images(config\u001B[38;5;241m.\u001B[39mMASK_DATASET_PATH)))\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# partition the data into training and testing splits using 85% of\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# the data for training and the remaining 15% for testing\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'paths' is not defined"
     ]
    }
   ],
   "source": [
    "# sort and then load raw and annotation images\n",
    "raw_paths = sorted(list(paths.list_images(config.IMAGE_DATASET_PATH)))\n",
    "annotation_paths = sorted(list(paths.list_images(config.MASK_DATASET_PATH)))\n",
    "\n",
    "# partition the data into training and testing splits\n",
    "split = train_test_split(raw_paths,\n",
    "                         annotation_paths,\n",
    "                         test_size=config.TEST_SPLIT,\n",
    "                         random_state=42)\n",
    "\n",
    "(train_images, test_images) = split[:2]\n",
    "(train_annotations, test_annotations) = split[2:]\n",
    "\n",
    "# write the test image paths to disk so that we can use them when evaluating/testing our model later\n",
    "print(\"[INFO] saving testing image paths...\")\n",
    "with open(config.TEST_PATHS, \"w\") as f:\n",
    "    f.write(\"\\n\".join(test_images))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read the images into Tensors using our `SegmentationDataSet` class and then generate data loaders using `PyTorch`'s\n",
    "`DataLoader` module."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# define some transformations to later use when we load the dataset\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m transforms \u001B[38;5;241m=\u001B[39m \u001B[43mtransforms\u001B[49m\u001B[38;5;241m.\u001B[39mCompose([transforms\u001B[38;5;241m.\u001B[39mToPILImage(),\n\u001B[0;32m      3\u001B[0m                                  transforms\u001B[38;5;241m.\u001B[39mResize((config\u001B[38;5;241m.\u001B[39mINPUT_IMAGE_HEIGHT,\n\u001B[0;32m      4\u001B[0m                                                     config\u001B[38;5;241m.\u001B[39mINPUT_IMAGE_WIDTH)),\n\u001B[0;32m      5\u001B[0m                                  transforms\u001B[38;5;241m.\u001B[39mToTensor()])\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# create the train and test datasets\u001B[39;00m\n\u001B[0;32m      8\u001B[0m train_ds \u001B[38;5;241m=\u001B[39m SegmentationDataSet(image_paths\u001B[38;5;241m=\u001B[39mtrain_images,\n\u001B[0;32m      9\u001B[0m                                mask_paths\u001B[38;5;241m=\u001B[39mtrain_annotations,\n\u001B[0;32m     10\u001B[0m                                transform\u001B[38;5;241m=\u001B[39mtransforms)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "# define some transformations to later use when we load the dataset\n",
    "transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                 transforms.Resize((config.INPUT_IMAGE_HEIGHT,\n",
    "                                                    config.INPUT_IMAGE_WIDTH)),\n",
    "                                 transforms.ToTensor()])\n",
    "\n",
    "# loop through each strain and create a model and save it into our `output` folder\n",
    "for strain in config.ALGAE_SPECIES:\n",
    "    train_image_subset, train_annotation_subset = get_dataset(strain, train_images, train_annotations)\n",
    "    # create the train dataset\n",
    "    train_ds = SegmentationDataSet(image_paths=train_image_subset,\n",
    "                                   mask_paths=train_annotation_subset,\n",
    "                                   transform=transform)\n",
    "    test_image_subset, test_annotation_subset = get_dataset(strain, test_images, test_annotations)\n",
    "    # create the test dataset\n",
    "    test_ds = SegmentationDataSet(image_paths=test_image_subset,\n",
    "                                  mask_paths=test_annotation_subset,\n",
    "                                  transform=transform)\n",
    "\n",
    "    print(f\"[INFO] found {len(train_ds)} samples in the training set...\")\n",
    "    print(f\"[INFO] found {len(test_ds)} samples in the test set...\")\n",
    "\n",
    "    # create the training data loaders\n",
    "    train_loader = DataLoader(train_ds,\n",
    "                              shuffle=True,\n",
    "                              batch_size=config.BATCH_SIZE,\n",
    "                              pin_memory=config.PIN_MEMORY,\n",
    "                              num_workers=os.cpu_count())\n",
    "    # create the test data loaders\n",
    "    test_loader = DataLoader(test_ds, shuffle=False,\n",
    "                             batch_size=config.BATCH_SIZE,\n",
    "                             pin_memory=config.PIN_MEMORY,\n",
    "                             num_workers=os.cpu_count())\n",
    "\n",
    "    # initialize our UNet model\n",
    "    unet = UNet().to(config.DEVICE)\n",
    "\n",
    "    lossFunc = BCEWithLogitsLoss()  # initialize loss function\n",
    "    optimisation = Adam(unet.parameters(), lr=config.INIT_LR)  # initialize optimiser\n",
    "\n",
    "    # calculate steps per epoch for training and test set\n",
    "    train_steps = len(train_ds) // config.BATCH_SIZE\n",
    "    test_steps = len(test_ds) // config.BATCH_SIZE\n",
    "\n",
    "    history = {\"train_loss\": [], \"test_loss\": []}  # initialize a dictionary to store training history\n",
    "\n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=config.PATIENCE,\n",
    "                                   verbose=True,\n",
    "                                   path=config.MODEL_PATH+'.{}'.format(strain))\n",
    "\n",
    "    print(\"[INFO] training the network for {} algae strain...\".format(strain))\n",
    "    start_time = time.time()\n",
    "    for e in tqdm(range(config.NUM_EPOCHS)):\n",
    "        # set the model in training mode\n",
    "        unet.train()\n",
    "        # initialize the total training and validation loss\n",
    "        total_train_loss = 0\n",
    "        total_test_loss = 0\n",
    "\n",
    "        # loop over the training set\n",
    "        for (i, (x, y)) in enumerate(train_loader):\n",
    "            (x, y) = (x.to(config.DEVICE), y.to(config.DEVICE))\n",
    "            # perform a forward pass and calculate the training loss\n",
    "            pred = unet(x)\n",
    "            loss = lossFunc(pred, y)\n",
    "            optimisation.zero_grad()  # reset gradient\n",
    "            loss.backward()  # compute backprop\n",
    "            optimisation.step()  # update model parameters\n",
    "            total_train_loss += loss  # add the loss to the total training loss\n",
    "\n",
    "        # switch off autograd\n",
    "        with torch.no_grad():\n",
    "            unet.eval()  # set the model in evaluation mode\n",
    "            for (x, y) in test_loader:\n",
    "                (x, y) = (x.to(config.DEVICE), y.to(config.DEVICE))\n",
    "                pred = unet(x)  # make predictions\n",
    "                total_test_loss += lossFunc(pred, y)  # get validation loss\n",
    "\n",
    "        # calculate the average training and validation loss\n",
    "        avg_train_loss = total_train_loss / train_steps\n",
    "        avg_test_loss = total_test_loss / test_steps\n",
    "\n",
    "        # update our training history\n",
    "        history[\"train_loss\"].append(avg_train_loss.cpu().detach().numpy())\n",
    "        history[\"test_loss\"].append(avg_test_loss.cpu().detach().numpy())\n",
    "\n",
    "        # print the model training and validation information\n",
    "        print(\"[INFO] EPOCH: {}/{}\".format(e + 1, config.NUM_EPOCHS))\n",
    "        print(\"Train loss: {:.6f}, Test loss: {:.4f}\".format(avg_train_loss, avg_test_loss))\n",
    "\n",
    "        # see if early stopping gets triggered\n",
    "        early_stopping(avg_test_loss, unet)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"[INFO] total time taken to train the model: {:.2f}s\".format(end_time - start_time))\n",
    "\n",
    "\n",
    "    \"\"\" Once training is done, we can plot the loss for each strain and see how our model performed.\n",
    "    We also save each model to use later for inference \"\"\"\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure()\n",
    "    plt.plot(history[\"train_loss\"], label=\"train_loss\")\n",
    "    plt.plot(history[\"test_loss\"], label=\"test_loss\")\n",
    "    plt.title(\"Training Loss for {}\".format(strain))\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc=\"upper right\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}